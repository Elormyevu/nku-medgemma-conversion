name: GGUF Q2_K Quantization + HF Upload

on:
  workflow_dispatch:

jobs:
  quantize-and-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Maximize disk space
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/share/boost
          df -h
          
      - name: Setup llama.cpp
        run: |
          git clone https://github.com/ggml-org/llama.cpp
          cd llama.cpp
          cmake -B build
          cmake --build build --config Release -j$(nproc)
          
      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install transformers accelerate sentencepiece huggingface_hub
          
      - name: Download and Quantize to Q2_K
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "[1/4] Downloading MedGemma 4B..."
          python -c "
          import os
          from huggingface_hub import snapshot_download, login
          login(token=os.environ['HF_TOKEN'])
          snapshot_download(repo_id='google/medgemma-1.5-4b-it', local_dir='./medgemma-model')
          print('[OK] Download complete')
          "
          
          echo "[2/4] Converting to GGUF (F16)..."
          cd llama.cpp
          python convert_hf_to_gguf.py ../medgemma-model \
            --outfile ../medgemma-4b-f16.gguf \
            --outtype f16
          
          echo "[3/4] Quantizing to Q2_K (2-bit for budget phones)..."
          ./build/bin/llama-quantize ../medgemma-4b-f16.gguf ../medgemma-4b-q2_k.gguf Q2_K
          
          echo "[4/4] Checking file sizes..."
          cd ..
          ls -lh *.gguf
          
          # Clean up
          rm -f medgemma-4b-f16.gguf
          rm -rf medgemma-model
          
          echo "[SUCCESS] Q2_K quantization complete!"
          
      - name: Upload Q2_K to HuggingFace
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Create repo if doesn't exist and upload Q2_K model
          python -c "
          import os
          from huggingface_hub import HfApi, create_repo
          
          token = os.environ['HF_TOKEN']
          api = HfApi(token=token)
          
          # Create repo
          try:
              create_repo('wredd/medgemma-4b-gguf', repo_type='model', exist_ok=True, token=token)
              print('[OK] Repo created or exists')
          except Exception as e:
              print(f'Repo creation note: {e}')
          
          # Upload Q2_K model
          api.upload_file(
              path_or_fileobj='medgemma-4b-q2_k.gguf',
              path_in_repo='medgemma-4b-q2_k.gguf',
              repo_id='wredd/medgemma-4b-gguf',
              commit_message='Add MedGemma 4B Q2_K (2-bit) for budget Android phones'
          )
          print('[SUCCESS] Q2_K uploaded to HuggingFace!')
          "
          
      - name: Download Q4_K_M from Run #9 and Upload
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Download existing Q4_K_M artifact
          gh run download 21610576543 --name medgemma-gguf-int4 --dir ./q4_model
          ls -lh ./q4_model/
          
          # Upload Q4_K_M model
          python -c "
          import os
          from huggingface_hub import HfApi
          api = HfApi(token=os.environ['HF_TOKEN'])
          api.upload_file(
              path_or_fileobj='q4_model/medgemma-4b-q4_k_m.gguf',
              path_in_repo='medgemma-4b-q4_k_m.gguf',
              repo_id='wredd/medgemma-4b-gguf',
              commit_message='Add MedGemma 4B Q4_K_M (INT4) for standard devices'
          )
          print('[SUCCESS] Q4_K_M uploaded to HuggingFace!')
          "
          
      - name: Create Model Card
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          cat > README.md << 'EOF'
          ---
          license: gemma
          base_model: google/medgemma-1.5-4b-it
          tags:
            - medical
            - gguf
            - quantized
            - llama.cpp
          ---
          
          # MedGemma 4B GGUF Quantized Models
          
          Quantized versions of [google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it) for on-device medical AI.
          
          ## Available Models
          
          | File | Quantization | Size | RAM Needed | Use Case |
          |------|--------------|------|------------|----------|
          | `medgemma-4b-q4_k_m.gguf` | Q4_K_M (4-bit) | ~2.4GB | ~4GB | Standard phones |
          | `medgemma-4b-q2_k.gguf` | Q2_K (2-bit) | ~1.4GB | ~2.5GB | Budget phones (<$80) |
          
          ## Usage with llama.cpp
          
          ```bash
          ./llama-cli -m medgemma-4b-q2_k.gguf -p "What are common symptoms of malaria?"
          ```
          
          ## License
          
          Subject to [Gemma Terms of Use](https://ai.google.dev/gemma/terms).
          EOF
          
          python -c "
          import os
          from huggingface_hub import HfApi
          api = HfApi(token=os.environ['HF_TOKEN'])
          api.upload_file(
              path_or_fileobj='README.md',
              path_in_repo='README.md',
              repo_id='wredd/medgemma-4b-gguf',
              commit_message='Add model card'
          )
          print('[SUCCESS] Model card uploaded!')
          "
          
      - name: Upload Q2_K Artifact
        uses: actions/upload-artifact@v4
        with:
          name: medgemma-gguf-q2_k
          path: medgemma-4b-q2_k.gguf
          retention-days: 30
