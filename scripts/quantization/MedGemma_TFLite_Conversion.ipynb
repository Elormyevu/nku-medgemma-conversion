{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MedGemma 1.5 4B ‚Üí TFLite Conversion\n",
        "\n",
        "This notebook converts MedGemma 1.5 4B to INT8 quantized TFLite format.\n",
        "\n",
        "**Requirements:**\n",
        "- HuggingFace account with access to MedGemma\n",
        "- HuggingFace token\n",
        "\n",
        "**Runtime:** Use GPU runtime (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html 2>/dev/null || pip install -q torch_xla\n",
        "!pip install -q \"transformers>=4.44.0,<4.46.0\" accelerate tqdm\n",
        "!pip install -q ai-edge-torch-nightly tf-nightly\n",
        "!pip install -q kagglehub sentencepiece protobuf\n",
        "!pip uninstall -y torchao 2>/dev/null || true\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Login to HuggingFace\n",
        "from huggingface_hub import login\n",
        "login()  # Enter your HuggingFace token when prompted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Conversion Script\n",
        "import sys\n",
        "import os\n",
        "import types\n",
        "from unittest.mock import MagicMock\n",
        "import importlib.machinery\n",
        "\n",
        "# Detect if we have real torch_xla\n",
        "def has_real_torch_xla():\n",
        "    try:\n",
        "        import torch_xla\n",
        "        return hasattr(torch_xla, '__file__') and torch_xla.__file__ is not None\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "USE_REAL_XLA = has_real_torch_xla()\n",
        "print(f\"üîß torch_xla available: {USE_REAL_XLA}\")\n",
        "\n",
        "if not USE_REAL_XLA:\n",
        "    print(\"‚ö†Ô∏è Mocking torch_xla...\")\n",
        "    class AttributeAwareMock(types.ModuleType):\n",
        "        def __init__(self, name):\n",
        "            super().__init__(name)\n",
        "            self.__path__ = []\n",
        "            self.__spec__ = importlib.machinery.ModuleSpec(name, None)\n",
        "        def __getattr__(self, name):\n",
        "            return MagicMock(name=f\"mock.{name}\")\n",
        "\n",
        "    class ForgeryMock(AttributeAwareMock):\n",
        "        def exported_program_to_stablehlo(self, *args, **kwargs):\n",
        "            bundle = MagicMock()\n",
        "            bundle.state_dict = {}\n",
        "            bundle.additional_constants = []\n",
        "            mock_func = MagicMock()\n",
        "            mock_func.meta.name = \"forward\"\n",
        "            mock_func.meta.input_locations = []\n",
        "            mock_func.meta.input_signature = []\n",
        "            mock_func.meta.output_signature = [MagicMock(dtype=\"float32\", shape=[1, 256000])]\n",
        "            mock_func.bytecode = b\"\"\n",
        "            bundle.stablehlo_funcs = [mock_func]\n",
        "            result = MagicMock()\n",
        "            result._bundle = bundle\n",
        "            return result\n",
        "        def merge_stablehlo_bundles(self, *args, **kwargs):\n",
        "            gm = MagicMock()\n",
        "            gm._bundle = args[0][0] if args and args[0] else MagicMock()\n",
        "            return gm\n",
        "\n",
        "    mock_xla = AttributeAwareMock(\"torch_xla\")\n",
        "    shlo_forgery = ForgeryMock(\"torch_xla.stablehlo\")\n",
        "    mock_xla.stablehlo = shlo_forgery\n",
        "    mock_xla.core = AttributeAwareMock(\"torch_xla.core\")\n",
        "    mock_xla.utils = AttributeAwareMock(\"torch_xla.utils\")\n",
        "    mock_xla.experimental = AttributeAwareMock(\"torch_xla.experimental\")\n",
        "    sys.modules[\"torch_xla\"] = mock_xla\n",
        "    sys.modules[\"torch_xla.core\"] = mock_xla.core\n",
        "    sys.modules[\"torch_xla.core.xla_model\"] = AttributeAwareMock(\"torch_xla.core.xla_model\")\n",
        "    sys.modules[\"torch_xla.utils\"] = mock_xla.utils\n",
        "    sys.modules[\"torch_xla.utils.utils\"] = AttributeAwareMock(\"torch_xla.utils.utils\")\n",
        "    sys.modules[\"torch_xla.experimental\"] = mock_xla.experimental\n",
        "    sys.modules[\"torch_xla.experimental.xla_marker\"] = AttributeAwareMock(\"torch_xla.experimental.xla_marker\")\n",
        "    sys.modules[\"torch_xla.experimental.xla_mlir_debuginfo\"] = AttributeAwareMock(\"torch_xla.experimental.xla_mlir_debuginfo\")\n",
        "    sys.modules[\"torch_xla.experimental.mark_pattern_utils\"] = AttributeAwareMock(\"torch_xla.experimental.mark_pattern_utils\")\n",
        "    sys.modules[\"torch_xla.stablehlo\"] = shlo_forgery\n",
        "\n",
        "import torch\n",
        "\n",
        "# Register XLA ops\n",
        "try:\n",
        "    from torch.library import Library, impl\n",
        "    lib = Library(\"xla\", \"DEF\")\n",
        "    lib.define(\"mark_tensor(Tensor self) -> Tensor\")\n",
        "    lib.define(\"write_mlir_debuginfo(Tensor self, Tensor other, int index) -> Tensor\")\n",
        "    @impl(lib, \"mark_tensor\", \"CompositeExplicitAutograd\")\n",
        "    def mark_tensor(self): return self\n",
        "    @impl(lib, \"write_mlir_debuginfo\", \"CompositeExplicitAutograd\")\n",
        "    def write_mlir_debuginfo(self, other, idx): return self\n",
        "except Exception as e:\n",
        "    print(f\"Info: XLA ops: {e}\")\n",
        "\n",
        "from transformers import AutoModelForImageTextToText\n",
        "import ai_edge_torch\n",
        "\n",
        "# Patch PassBase\n",
        "try:\n",
        "    from torch.fx.passes.infra.pass_base import PassBase, PassResult\n",
        "    original_pass_call = PassBase.__call__\n",
        "    def patched_pass_call(self, *args, **kwargs):\n",
        "        if type(self).__name__ == \"InjectMlirDebuginfoPass\":\n",
        "            gm = args[0] if args else kwargs.get('graph_module')\n",
        "            return PassResult(gm, True)\n",
        "        return original_pass_call(self, *args, **kwargs)\n",
        "    PassBase.__call__ = patched_pass_call\n",
        "    print(\"‚úÖ PassBase patched\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "\n",
        "# Patch autocast\n",
        "class dummy_autocast:\n",
        "    def __init__(self, *a, **kw): pass\n",
        "    def __enter__(self): pass\n",
        "    def __exit__(self, *a): pass\n",
        "    def __call__(self, f): return f\n",
        "torch.autocast = dummy_autocast\n",
        "if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'amp'):\n",
        "    torch.cuda.amp.autocast = dummy_autocast\n",
        "if hasattr(torch, 'amp'):\n",
        "    torch.amp.autocast = dummy_autocast\n",
        "\n",
        "from ai_edge_torch.generative.quantize import quant_recipes, quant_recipe_utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Patch Gemma 3\n",
        "try:\n",
        "    from transformers import masking_utils\n",
        "    simple_mapping = {\"eager\": \"eager\", \"sdpa\": \"sdpa\", \"flash_attention_2\": \"flash_attention_2\"}\n",
        "    if hasattr(masking_utils, \"ALL_MASK_ATTENTION_FUNCTIONS\"):\n",
        "        masking_utils.ALL_MASK_ATTENTION_FUNCTIONS._global_mapping = simple_mapping\n",
        "    def dummy_create_causal_mask(input_ids=None, *args, **kwargs):\n",
        "        device = torch.device('cpu')\n",
        "        seq_len = input_ids.shape[-1] if input_ids is not None else 1\n",
        "        return torch.tril(torch.ones((seq_len, seq_len), device=device, dtype=torch.bool))[None, None, :, :]\n",
        "    masking_utils.create_causal_mask = dummy_create_causal_mask\n",
        "    masking_utils.create_sliding_window_causal_mask = dummy_create_causal_mask\n",
        "    import transformers.models.gemma3.modeling_gemma3 as g3_mod\n",
        "    def TraceableOutput(**kwargs):\n",
        "        val = kwargs.get('logits', kwargs.get('last_hidden_state'))\n",
        "        if val is None and kwargs:\n",
        "            val = next(iter(kwargs.values()))\n",
        "        return (val,) if val is not None else ()\n",
        "    g3_mod.BaseModelOutputWithPast = TraceableOutput\n",
        "    g3_mod.Gemma3ModelOutputWithPast = TraceableOutput\n",
        "    g3_mod.Gemma3CausalLMOutputWithPast = TraceableOutput\n",
        "    def patched_rope_forward(self, x, position_ids):\n",
        "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
        "        position_ids_expanded = position_ids[:, None, :].float()\n",
        "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        cos = emb.cos() * self.attention_scaling\n",
        "        sin = emb.sin() * self.attention_scaling\n",
        "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "    g3_mod.Gemma3RotaryEmbedding.forward = patched_rope_forward\n",
        "    print(\"‚úÖ Gemma 3 patches applied\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "\n",
        "class LanguageModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model.language_model if hasattr(model, 'language_model') else model\n",
        "    def forward(self, input_ids):\n",
        "        outputs = self.model(input_ids=input_ids, use_cache=False)\n",
        "        if isinstance(outputs, (list, tuple)) and len(outputs) > 0:\n",
        "            return outputs[0]\n",
        "        return outputs\n",
        "\n",
        "print(\"‚úÖ Conversion script ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Run Conversion\n",
        "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
        "OUTPUT_PATH = \"/content/medgemma_int4.tflite\"\n",
        "\n",
        "print(f\"üöÄ Converting {MODEL_ID}...\")\n",
        "pbar = tqdm(total=100, desc=\"Progress\")\n",
        "\n",
        "# Load model\n",
        "pbar.set_description(\"Loading model\")\n",
        "pbar.update(5)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float32,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "model.eval()\n",
        "pbar.update(10)\n",
        "\n",
        "# Wrap model\n",
        "lm_wrapper = LanguageModelWrapper(model)\n",
        "dummy_input = torch.randint(0, 256000, (1, 64))\n",
        "pbar.update(5)\n",
        "\n",
        "# Configure quantization\n",
        "pbar.set_description(\"Configuring quantization\")\n",
        "try:\n",
        "    quant_cfg = quant_recipes.full_linear_int8_dynamic_recipe()\n",
        "    print(\"\\n‚úÖ Using INT8 quantization\")\n",
        "except:\n",
        "    quant_cfg = quant_recipe_utils.create_config_from_recipe(\n",
        "        quant_recipe_utils.create_layer_quant_fp16()\n",
        "    )\n",
        "    print(\"\\n‚ö†Ô∏è Falling back to FP16\")\n",
        "pbar.update(5)\n",
        "\n",
        "# Convert\n",
        "pbar.set_description(\"Converting to TFLite\")\n",
        "with torch.no_grad():\n",
        "    edge_model = ai_edge_torch.convert(\n",
        "        lm_wrapper,\n",
        "        (dummy_input,),\n",
        "        quant_config=quant_cfg\n",
        "    )\n",
        "pbar.update(60)\n",
        "\n",
        "# Export\n",
        "pbar.set_description(\"Exporting\")\n",
        "edge_model.export(OUTPUT_PATH)\n",
        "pbar.update(15)\n",
        "pbar.close()\n",
        "\n",
        "size_gb = os.path.getsize(OUTPUT_PATH) / (1024**3)\n",
        "print(f\"\\n‚úÖ Conversion complete!\")\n",
        "print(f\"üìÅ Output: {OUTPUT_PATH}\")\n",
        "print(f\"üì¶ Size: {size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Download the converted model\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_PATH)"
      ]
    }
  ]
}
