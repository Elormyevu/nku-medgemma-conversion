name: MedGemma IQ1_M Extreme Compression

on:
  workflow_dispatch:

jobs:
  compress-iq1m:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Maximize disk space
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/share/boost /opt/hostedtoolcache
          df -h
          
      - name: Setup llama.cpp
        run: |
          git clone https://github.com/ggml-org/llama.cpp
          cd llama.cpp
          cmake -B build -DGGML_NATIVE=OFF
          cmake --build build --config Release -j$(nproc)
          echo "llama.cpp built successfully"
          
      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install transformers accelerate sentencepiece huggingface_hub
          
      - name: Download MedGemma
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "[1/5] Downloading MedGemma 4B from google/medgemma-4b-it..."
          python -c "
          import os
          from huggingface_hub import snapshot_download, login
          login(token=os.environ['HF_TOKEN'])
          snapshot_download(repo_id='google/medgemma-4b-it', local_dir='./medgemma-model')
          print('[OK] Download complete')
          "
          
      - name: Convert to F16 GGUF
        run: |
          echo "[2/5] Converting to GGUF (F16) for ideal imatrix quality..."
          cd llama.cpp
          python convert_hf_to_gguf.py ../medgemma-model \
            --outfile ../medgemma-4b-f16.gguf \
            --outtype f16
          cd ..
          ls -lh *.gguf
          
      - name: Generate Medical Importance Matrix
        run: |
          echo "[3/5] Generating medical importance matrix..."
          # Use comprehensive African primary care calibration (243 lines, 14+ languages)
          cat calibration/african_primary_care.txt > calibration_data.txt
          
          echo "Calibration file stats:"
          echo "Lines: $(wc -l < calibration_data.txt)"
          echo "Chars: $(wc -c < calibration_data.txt)"
          
          # Generate imatrix - critical for IQ1_M quality
          cd llama.cpp
          ./build/bin/llama-imatrix \
            -m ../medgemma-4b-f16.gguf \
            -f ../calibration_data.txt \
            -o ../medical.imatrix \
            --chunks 64
          cd ..
          echo "[OK] Medical imatrix generated"
          ls -lh medical.imatrix
          
      - name: Quantize with imatrix IQ1_M
        run: |
          echo "[4/5] Quantizing to IQ1_M with medical imatrix..."
          echo "This is EXTREME compression - preserving CHW-critical weights only"
          cd llama.cpp
          ./build/bin/llama-quantize \
            --imatrix ../medical.imatrix \
            ../medgemma-4b-f16.gguf \
            ../medgemma-4b-iq1_m.gguf \
            IQ1_M
          cd ..
          echo "[OK] IQ1_M compression complete!"
          ls -lh *.gguf
          
          # Clean up intermediate files
          rm -f medgemma-4b-f16.gguf
          rm -rf medgemma-model
          
      - name: Upload to HuggingFace
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "[5/5] Uploading to HuggingFace..."
          python -c "
          import os
          from huggingface_hub import HfApi, create_repo
          
          token = os.environ['HF_TOKEN']
          api = HfApi(token=token)
          
          try:
              create_repo('wredd/medgemma-4b-gguf', repo_type='model', exist_ok=True, token=token)
          except Exception as e:
              print(f'Repo note: {e}')
          
          api.upload_file(
              path_or_fileobj='medgemma-4b-iq1_m.gguf',
              path_in_repo='medgemma-4b-iq1_m.gguf',
              repo_id='wredd/medgemma-4b-gguf',
              commit_message='Add MedGemma IQ1_M - Extreme compression with African healthcare imatrix (~0.7GB)'
          )
          print('[SUCCESS] IQ1_M uploaded!')
          "
          
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: medgemma-iq1m
          path: medgemma-4b-iq1_m.gguf
          retention-days: 30
